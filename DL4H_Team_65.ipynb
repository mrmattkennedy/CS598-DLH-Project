{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Repo URL:\n",
        "https://github.com/mrmattkennedy/CS598-DLH-Project\n",
        "\n",
        "# Team 65 members:\n",
        "Matthew Kennedy"
      ],
      "metadata": {
        "id": "qmSXFXezfSUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This study focuses on predicting 30-days mortality for MIMIC-III patients diagnosed with sepsis-3 using the XGBoost algorithm. This is a predictive modeling problem aiming to improve mortality prediction over traditional methods. The importance of solving this problem is aimed at tackling sepsis, which is a major cause of mortality especially in ICU patients. Early and accurate prediction of mortality in these patients is important as it can guide time-critical and appropriate treatment for patients, potentially improving survival outcomes. This study aims to enhance the predictive accuracy using machine learning, which could provide clinicians with a powerful tool for risk assessment and management.\n",
        "\n",
        "The complexity of sepsis, including its vague syndrome definitions, unknown sources of infection, as well as high mortality rates, makes establishing a reliable and effective prognostic model challenging. Traditional machine learning models, which are based on small sample sizes or simplistic statistical assumptions, are limited in their predictive power. Traditional methods for diagnosing sepsis include the use of serum markers and scoring systems like APHACHE-II and SAPS-II. However, these have limitations in sensitivity, specificity, and the ability to handle complex interactions within data. XGBoost has shown improved predictive performance by efficiently handling missing data and assembling weak prediction models to create a more accurate composite model.\n",
        "\n",
        "L. et al. [1] proposed a machine learning model using the XGBoost algorithm to predict 30-day mortality among patients with sepsis-3 in the MIMIC-III dataset, comparing its performance against traditional logistic regression and SAPS-II score models. The innovation for this study is found in applying the XGBoost algorithm, known for its efficiency with missing data and capability to enhance predictive accuracy by combining weak models. This study demonstrates the superiority of XGBoost over conventional methods in the context of sepsis mortality prediction.\n",
        "\n",
        "For post-results analysis and scoring, tthe XGBoost model showed superior performance with higher AUCs compared to traditional logistic regression and SAPS-II models, indicating better predictive accuracy. This study significantly contributes to the sepsis research field by showcasing the application of a machine learning algorithm to accurately predict mortality, potentially aiding clinicians in making informed decisions and tailoring patient management strategies effectively.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "1.   Hypothesis 1: The XGBoost algorithm can outperform traditional logistic regression and SAPS-II scoring models in predicting 30-day mortality among patients with sepsis-3 based on scoring metrics.\n",
        "* Corresponding experiment: Create a logistic regression model, a SAPS-II dataset, and an XGBoost model, and use the features specified by L. et al. [1] for these models. Then, analyze the results of both models in predicting 30-day mortality due to sepsis-3 and compare.\n",
        "2.   Hypothesis 2: The set of features identified by the XGBoost model as significant predictors of 30-day mortality are significant compared to features not used\n",
        "* Corresponding experiment: Perform several ablations, including dropping several features and adding several others, and compare post-training scores across different models to see which features area most important in each model's decisions, and how effective each model is.\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery\n",
        "!pip install pandas\n",
        "!pip install db-dtypes\n",
        "!pip install scikit-learn\n",
        "!pip install xgboost\n",
        "!pip install opencv-python\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "J_4oWML80LwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to check if google colab or not to properly show images\n",
        "import sys\n",
        "GOOGLE_COLAB ='google.colab' in sys.modules\n",
        "print('Using Google Colab: ', GOOGLE_COLAB)\n",
        "\n",
        "if GOOGLE_COLAB:\n",
        "  from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "dQBwt5ehicYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import json\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Set env variables\n",
        "os.environ[\"GCLOUD_PROJECT\"] = 'dlh-project-418923'\n",
        "os.environ[\"BQ_DATASET_BASE\"] = 'mimiciii_clinical'\n",
        "os.environ[\"BQ_DATASET_DERIVED\"] = 'mimiciii_derived'\n",
        "os.environ[\"BQ_DATASET_FEATURES\"] = 'models_features'\n",
        "os.environ[\"LOG_REG_FEATURES_VIEW\"] = 'logreg_features'\n",
        "os.environ[\"XGBOOST_FEATURES_VIEW\"] = 'xgboost_features'\n",
        "os.environ[\"SAPSII_FEATURES_VIEW\"] = 'sapsii_features'\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# URLs for downloading from drive\n",
        "gcloud_ids = {'sa-creds': '1qNBEkuIQgj0K-PJ9ETvqF6Cel8wMlVGA',\n",
        "              'mimic-views': '13uCK5Go9XvANkeujrDaWY2cK3ocWakVM',\n",
        "              'model-features': '1_z6HLDKnHH8iUddS_wMPg3UhexhczpZq',\n",
        "              'img-features': '1VTAdcxG0Tz6vLhRWhc2b_kWaHjfeK_am',\n",
        "              'graph-roc': '1Hc_vLhGYR4yU4LG-o42p3rVdUEDHXAuR',\n",
        "              'graph-dca': '1q6c4b_IWql7hp829K1dDaoS6GEP6Bhcv',\n",
        "              'graph-cic': '1x56v9kFDM_dS3JcML3UL0BlOSoJr7XGx',\n",
        "              'graph-nomogram': '1yyFhCauWCAgqRyJe5E-DZdkdfiJiOiIC',\n",
        "              'my-roc': '1LHFSwLI1X5fOCVtPClQr9od6c3ZfD4GD',\n",
        "              'my-dca': '1mVIHMui69o3SRv76cbrrm9CnnbsT7zWn',}\n",
        "gcloud_url = 'https://drive.google.com/uc?export=download&id={}'"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to show images based on URL\n",
        "def show_image(gcloud_id: str, resize: bool=False) -> None:\n",
        "  # Download bytes from URL\n",
        "  f = io.BytesIO()\n",
        "  # Write to BytesIO and reset stream position\n",
        "  f.write(requests.get(gcloud_url.format(gcloud_ids[gcloud_id])).content)\n",
        "  f.seek(0)\n",
        "  # Write bytes into numpy array, then to a cv2 image\n",
        "  file_bytes = np.asarray(bytearray(f.read()), dtype=np.uint8)\n",
        "  img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
        "  # Resize if specified\n",
        "  if resize: img=cv2.resize(img, (800,600))\n",
        "  # Close bytesIO\n",
        "  f.close()\n",
        "\n",
        "  # Show image\n",
        "  if GOOGLE_COLAB: cv2_imshow(img)\n",
        "  else: cv2.imshow(img)"
      ],
      "metadata": {
        "id": "pqnTBJfv9NBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source\n",
        "The data used in this project is the MIMIC III dataset. However, due to the size of the original dataset, a subsample of roughly half of the raw original dataset was loaded into Google Bigquery. The reason BQ was chosen was due to the availability of [this GitHub repo from MIT](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iii) that has easily creatable views and analyses of the MIMIC datasets, and can be processed via BQ.\n",
        "\n",
        "The data is split into 3 datasets in BQ:\n",
        "\n",
        "1. **mimiciii_clinical**: All of the original, raw data. Data was sourced from [Physionet](https://physionet.org/content/mimiciii/1.4/)\n",
        "2. **mimiciii_derived**: Any additional views created from the raw data. SQL for these views was sourced from the [mimic-code library](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iii/concepts) mentioned above\n",
        "3. **mimiciii_features**: Views created that contain the features for each type of model. These features were chosen from L. et al. [1][p.8] based on the author's chose features, which were, \"identifed by the results of backward stepwise analysis and strongly associated with mortality in 30 days\"."
      ],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistics\n",
        "### Tables and views\n",
        "Tables contained in 'dlh-project-418923.mimiciii_clinical':\n",
        "* dlh-project-418923.mimiciii_clinical.admissions, 58976 rows\n",
        "* dlh-project-418923.mimiciii_clinical.callout, 34499 rows\n",
        "* dlh-project-418923.mimiciii_clinical.caregivers, 7567 rows\n",
        "* dlh-project-418923.mimiciii_clinical.chartevents, 330712483 rows\n",
        "* dlh-project-418923.mimiciii_clinical.cptevents, 573146 rows\n",
        "* dlh-project-418923.mimiciii_clinical.d_cpt, 134 rows\n",
        "* dlh-project-418923.mimiciii_clinical.d_icd_diagnoses, 14567 rows\n",
        "* dlh-project-418923.mimiciii_clinical.d_icd_procedures, 3882 rows\n",
        "* dlh-project-418923.mimiciii_clinical.d_items, 12487 rows\n",
        "* dlh-project-418923.mimiciii_clinical.d_labitems, 753 rows\n",
        "* dlh-project-418923.mimiciii_clinical.datetimeevents, 4285647 rows\n",
        "* dlh-project-418923.mimiciii_clinical.diagnoses_icd, 651047 rows\n",
        "* dlh-project-418923.mimiciii_clinical.drgcodes, 115557 rows\n",
        "* dlh-project-418923.mimiciii_clinical.icustays, 61532 rows\n",
        "* dlh-project-418923.mimiciii_clinical.inputevents_cv, 14527935 rows\n",
        "* dlh-project-418923.mimiciii_clinical.inputevents_mv, 3218991 rows\n",
        "* dlh-project-418923.mimiciii_clinical.labevents, 23851932 rows\n",
        "* dlh-project-418923.mimiciii_clinical.microbiologyevents, 631726 rows\n",
        "* dlh-project-418923.mimiciii_clinical.noteevents, 2083180 rows\n",
        "* dlh-project-418923.mimiciii_clinical.outputevents, 4349218 rows\n",
        "* dlh-project-418923.mimiciii_clinical.patients, 46520 rows\n",
        "* dlh-project-418923.mimiciii_clinical.prescriptions, 4156450 rows\n",
        "* dlh-project-418923.mimiciii_clinical.procedureevents_mv, 258066 rows\n",
        "* dlh-project-418923.mimiciii_clinical.procedures_icd, 231945 rows\n",
        "* dlh-project-418923.mimiciii_clinical.services, 73343 rows\n",
        "* dlh-project-418923.mimiciii_clinical.transfers, 261897 rows\n",
        "\n",
        "Views contained in 'dlh-project-418923.mimiciii_derived', as well as descriptions:\n",
        "* dlh-project-418923.mimiciii_derived.blood_gas_first_day -- Highest and lowest blood gas values in the first 24 hours of a patient's ICU stay.\n",
        "* dlh-project-418923.mimiciii_derived.blood_gas_first_day_arterial -- As above, but arterial blood gases only.\n",
        "* dlh-project-418923.mimiciii_derived.echo_data -- Text extracted from echocardiography reports using regular expressions.\n",
        "* dlh-project-418923.mimiciii_derived.elixhauser_ahrq_v37 -- Comorbidities in categories proposed by Elixhauser et al. AHRQ produced the mapping.\n",
        "* dlh-project-418923.mimiciii_derived.explicit_sepsis -- Explicitly coded sepsis (i.e. a list of patients with ICD-9 codes which refer to sepsis).\n",
        "* dlh-project-418923.mimiciii_derived.gcs_first_day -- Highest and lowest Glasgow Coma Scale in the first 24 hours of a patient's ICU stay.\n",
        "* dlh-project-418923.mimiciii_derived.labs_first_day -- Highest and lowest laboratory values in the first 24 hours of a patient's ICU stay.\n",
        "* dlh-project-418923.mimiciii_derived.sofa -- The Sequential Organ Failure Assessment (SOFA) scale.\n",
        "* dlh-project-418923.mimiciii_derived.urine_output_first_day -- Total urine output over the first 24 hours of a patient's ICU stay.\n",
        "* dlh-project-418923.mimiciii_derived.ventilation_classifications -- Classifies patient settings as implying mechanical ventilation.\n",
        "* dlh-project-418923.mimiciii_derived.ventilation_durations -- Start and stop times for mechanical ventilation.\n",
        "* dlh-project-418923.mimiciii_derived.vitals_first_day -- Highest and lowest vital signs in the first 24 hours of a patient's ICU stay.\n",
        "* dlh-project-418923.mimiciii_derived.sapsii -- Simplified Acute Physiology Score II (SAPS II)\n",
        "\n",
        "\n",
        "Views contained in 'dlh-project-418923.models_features', which are used for the 2 models in this analysis (logistic regression and XGBoost):\n",
        "* dlh-project-418923.models_features.logreg_features\n",
        "* dlh-project-418923.models_features.xgboost_features\n",
        "* dlh-project-418923.models_features.sapsii_features"
      ],
      "metadata": {
        "id": "Kfi11uHBeANP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features selected\n",
        "\n",
        "From L. et al. [1][p.4] regarding feature selection:\n",
        "\n",
        "\"Firstly, the conventional logistic regression model was conducted using these signifcant variables identifed by backward stepwise analysis with Chi-square test. Then we chose an entry probability of p<0.05 by the stepwise selection method. Secondly, in the construction of SAPS II model, we used these time-stamp variables to do prediction based on the methods provided by the original literature of SAPS II. Thirdly, we performed XGBoost model to analysis the contribution (gain) of each variable to 30-days mortality, at the same time, backward stepwise analysis was processed to select the variable with a threshold of p<0.05 according to the Akaike information criterion (AIC).\"\n",
        "\n",
        "Additionally, the below features are used to calculate SAPS-II:\n",
        "*  Age, GCS\n",
        "*  VITALS: Heart rate, systolic blood pressure, temperature\n",
        "*  FLAGS: ventilation/cpap\n",
        "*  IO: urine output\n",
        "*  LABS: PaO2/FiO2 ratio, blood urea nitrogen, WBC, potassium, sodium, HCO3\n",
        "\n",
        "Below is an image of which features were selected for both models\n"
      ],
      "metadata": {
        "id": "46TjHDwQibwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_image('img-features')"
      ],
      "metadata": {
        "id": "3-SB2br6jhbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features statistics\n",
        "The target classification of this analysis is to accurately predict if a patient will die within 30 days of their first visit from sepsis. From the subsample of data present in BQ, there are 877 positive records (patients who died within 30 days), and 1443 negative records (patients who did not die within 30 days). All features are numerical, either discrete or continuous.\n",
        "\n",
        "For the model hyperparameters, not much is described in the paper such as number of epochs, scoring functions, learning rates, etc, so for this paper we will use standard approaches to each, as the hypothesis tested in the paper are less focused on hyperparameter tuning, and more about model selection strengths."
      ],
      "metadata": {
        "id": "qdb3DWjTd6Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Again, not much is described in terms of data preparation and pre-processing - the focus of the paper is on parameter selection and model performance. Because of this, we will be using basic model preprocessing, which will involve the below steps:\n",
        "\n",
        "1. Download SQL for views, as well as authenticate BigQuery client\n",
        "2. Drop and reset derived and feature datasets\n",
        "3. Create views in BQ from raw data tables\n",
        "4. Create feature-selection views from all derived views and raw data tables\n",
        "5. Download data from the respective view for all models\n",
        "6. Drop records that are missing all non-target data\n",
        "7. Drop non-feature columns\n",
        "8. Feature normalization using [sklearn MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), in order to scale data between 0 and 1 and prevent negative values\n",
        "9. Create train and test splits for data"
      ],
      "metadata": {
        "id": "8lArFeMmeA_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# 1. Download SQL for views, as well as authenticate BigQuery client\n",
        "\n",
        "# Read SA credentials for BQ\n",
        "f = io.BytesIO()\n",
        "f.write(requests.get(gcloud_url.format(gcloud_ids['sa-creds'])).content)\n",
        "creds = json.loads(f.getvalue().decode())\n",
        "# Authorize and close\n",
        "client = bigquery.Client.from_service_account_info(creds)\n",
        "f.close()\n",
        "\n",
        "# Read view SQL files\n",
        "f = io.BytesIO()\n",
        "f.write(requests.get(gcloud_url.format(gcloud_ids['mimic-views'])).content)\n",
        "# Parse from memory into dict\n",
        "view_data = {}\n",
        "with zipfile.ZipFile(f, 'r') as zip:\n",
        "    for file in zip.namelist():\n",
        "        # Create file name and data from decoded zip bytes\n",
        "        fname = os.path.basename(file)[:-4].lower() #Last 4 chars are .sql\n",
        "        # Store into StringIO for pandas to read\n",
        "        data = zip.read(file).decode()\n",
        "        view_data[fname] = data\n",
        "f.close()\n",
        "\n",
        "\n",
        "# Download model feature SQL files\n",
        "f = io.BytesIO()\n",
        "f.write(requests.get(gcloud_url.format(gcloud_ids['model-features'])).content)\n",
        "# Parse from memory into dict\n",
        "model_views_data = {}\n",
        "with zipfile.ZipFile(f, 'r') as zip:\n",
        "    for file in zip.namelist():\n",
        "        # Create file name and data from decoded zip bytes\n",
        "        fname = os.path.basename(file)[:-4].lower() #Last 4 chars are .sql\n",
        "        # Store into StringIO for pandas to read\n",
        "        data = zip.read(file).decode()\n",
        "        model_views_data[fname] = data\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Drop and reset derived and feature datasets\n",
        "\n",
        "# Create datasets\n",
        "for dname in [os.environ[\"BQ_DATASET_DERIVED\"], os.environ[\"BQ_DATASET_FEATURES\"]]:\n",
        "    # Get a list of datasets\n",
        "    datasets = list(client.list_datasets())  # Make an API request.\n",
        "    project = client.project\n",
        "    dataset_id = \"{}.{}\".format(client.project, dname)\n",
        "\n",
        "    # Check if the dataset already exists\n",
        "    dataset_already_exists = dname in [d.dataset_id for d in datasets]\n",
        "\n",
        "    # Delete if it exists already\n",
        "    if dataset_already_exists:\n",
        "        client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)  # Make an API request.\n",
        "        print(\"Deleted dataset '{}'\".format(dataset_id))\n",
        "\n",
        "    # Create new dataset\n",
        "    dataset = bigquery.Dataset(dataset_id)\n",
        "    dataset.location = \"US\"\n",
        "\n",
        "    # Send the dataset to the API for creation, with an explicit timeout.\n",
        "    # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
        "    # exists within the project.\n",
        "    dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
        "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
      ],
      "metadata": {
        "id": "V048-KmA1KZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create views in BQ from raw data tables\n",
        "\n",
        "# Create views\n",
        "for view_name, sql in view_data.items():\n",
        "    if 'sofa' in view_name: continue #Save sofa for last\n",
        "    if 'sapsii' in view_name: continue #Save sapsii for last\n",
        "\n",
        "    # Specify view ID\n",
        "    view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], view_name)\n",
        "    view = bigquery.Table(view_id)\n",
        "    view.view_query = sql\n",
        "\n",
        "    # Make an API request to create the view.\n",
        "    view = client.create_table(view)\n",
        "    print(f\"Created {view.table_type}: {str(view.reference)}\")\n",
        "\n",
        "# Create sofa view\n",
        "view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], 'sofa')\n",
        "view = bigquery.Table(view_id)\n",
        "view.view_query = view_data['sofa']\n",
        "# Make an API request to create the view.\n",
        "view = client.create_table(view)\n",
        "print(f\"Created {view.table_type}: {str(view.reference)}\")\n",
        "\n",
        "\n",
        "# Create sapsii view\n",
        "view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_DERIVED\"], 'sapsii')\n",
        "view = bigquery.Table(view_id)\n",
        "view.view_query = view_data['sapsii']\n",
        "# Make an API request to create the view.\n",
        "view = client.create_table(view)\n",
        "print(f\"Created {view.table_type}: {str(view.reference)}\")"
      ],
      "metadata": {
        "id": "_e3wbQ7c1K9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Create feature-selection views from all derived views and raw data tables\n",
        "\n",
        "# Create model feature views\n",
        "for view_name, sql in model_views_data.items():\n",
        "    # Specify view ID\n",
        "    view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], view_name)\n",
        "    view = bigquery.Table(view_id)\n",
        "    view.view_query = sql\n",
        "\n",
        "    # Make an API request to create the view.\n",
        "    view = client.create_table(view)\n",
        "    print(f\"Created {view.table_type}: {str(view.reference)}\")"
      ],
      "metadata": {
        "id": "jnZ8BNZ-1NBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Download data from the respective view for all models\n",
        "\n",
        "# Get logistic regression model data\n",
        "view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"LOG_REG_FEATURES_VIEW\"])\n",
        "view = client.get_table(view_id)\n",
        "query_job = client.query(view.view_query)\n",
        "df_logreg = query_job.result().to_dataframe()\n",
        "print(f'Successfully downloaded data for logistic regression - shape {df_logreg.shape}')\n",
        "display(df_logreg.head(3))\n",
        "\n",
        "# Get XGBoost model data\n",
        "view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"XGBOOST_FEATURES_VIEW\"])\n",
        "view = client.get_table(view_id)\n",
        "query_job = client.query(view.view_query)\n",
        "df_xgb = query_job.result().to_dataframe()\n",
        "print(f'Successfully downloaded data for XGBoost - shape {df_xgb.shape}')\n",
        "display(df_xgb.head(3))\n",
        "\n",
        "# Get SAPS-II model data\n",
        "view_id = \"{}.{}.{}\".format(os.environ[\"GCLOUD_PROJECT\"], os.environ[\"BQ_DATASET_FEATURES\"], os.environ[\"SAPSII_FEATURES_VIEW\"])\n",
        "view = client.get_table(view_id)\n",
        "query_job = client.query(view.view_query)\n",
        "df_sapsii = query_job.result().to_dataframe()\n",
        "print(f'Successfully downloaded data for SAPS-II - shape {df_sapsii.shape}')\n",
        "display(df_sapsii.head(3))"
      ],
      "metadata": {
        "id": "beBmFYASf5Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Drop records that are missing all non-target data\n",
        "# For logistic regression, we cannot have any null values, so any records with null values will be dropped\n",
        "\n",
        "df_logreg.dropna(how='any', inplace=True)\n",
        "print(f'Successfully purged null values for logistic regression - shape {df_logreg.shape}')\n",
        "\n",
        "df_xgb.dropna(thresh=len(df_xgb.columns)-1, inplace=True)\n",
        "print(f'Successfully purged null values data for XGBoost - shape {df_xgb.shape}')\n",
        "\n",
        "df_sapsii.dropna(thresh=len(df_sapsii.columns)-1, inplace=True)\n",
        "print(f'Successfully purged null values data for SAPS-II - shape {df_sapsii.shape}')"
      ],
      "metadata": {
        "id": "xnaw2qMjgVNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Drop non-feature columns\n",
        "non_feature_cols = ['subject_id', 'hadm_id', 'icustay_id']\n",
        "\n",
        "df_logreg.drop(non_feature_cols, axis=1, inplace=True)\n",
        "df_xgb.drop(non_feature_cols, axis=1, inplace=True)\n",
        "df_sapsii.drop(non_feature_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "1RNGl4WwJ3qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Feature normalization using sklearn MinMaxScaler, in order to scale data between 0 and 1 and prevent negative values\n",
        "# We will not be scaling SAPS-II values since the probability is already calculcated, and that is what will be used for predicting.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scale logreg features\n",
        "scaler = MinMaxScaler()\n",
        "display(df_logreg.head(3))\n",
        "df_logreg[df_logreg.columns] = scaler.fit_transform(df_logreg[df_logreg.columns])\n",
        "display(df_logreg.head(3))\n",
        "\n",
        "# Scale xgb features\n",
        "scaler = MinMaxScaler()\n",
        "display(df_xgb.head(3))\n",
        "df_xgb[df_xgb.columns] = scaler.fit_transform(df_xgb[df_xgb.columns])\n",
        "display(df_xgb.head(3))"
      ],
      "metadata": {
        "id": "jTE0jHGYgcj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Create train and test splits for data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set test size to 15% of the dataset\n",
        "test_size = 0.1\n",
        "\n",
        "# Create logistic regression X and y data\n",
        "X_logreg = df_logreg.drop('target', axis=1)\n",
        "y_logreg = df_logreg[['target']]\n",
        "X_train_logreg, X_test_logreg, y_train_logreg, y_test_logreg = train_test_split(X_logreg, y_logreg, test_size=test_size, random_state=RANDOM_STATE)\n",
        "\n",
        "# Create xgboost X and y data\n",
        "X_xgb = df_xgb.drop('target', axis=1)\n",
        "y_xgb = df_xgb[['target']]\n",
        "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=test_size, random_state=RANDOM_STATE)\n",
        "\n",
        "# Create SAPSII X, y_true, and y_pred datasets\n",
        "X_sapsii = df_sapsii.drop('target', axis=1)\n",
        "y_true_sapsii = df_sapsii[['target']]\n",
        "y_pred_sapsii = df_sapsii['sapsii_prob']\n",
        "\n",
        "# Check sizes\n",
        "print('Logistic regression data shape\\n---------------')\n",
        "print(f'\\tX: {X_logreg.shape}')\n",
        "print(f'\\ty: {y_logreg.shape}')\n",
        "print()\n",
        "print('XGBoost data shape\\n---------------')\n",
        "print(f'\\tX: {X_xgb.shape}')\n",
        "print(f'\\ty: {y_xgb.shape}')\n",
        "print('SAPS-II data shape\\n---------------')\n",
        "print(f'\\ty: {df_sapsii.shape}')"
      ],
      "metadata": {
        "id": "9Zfj4ZA3olM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sizes can vary slightly between the number of valid records between models because the features selected for each model are different, so there can be more or less records with missing values between the two models."
      ],
      "metadata": {
        "id": "e-WSwYyOpUAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model summary\n",
        "These\n",
        "There are 3 models included in testing for this paper. They are not pretrained, and will be simply defined below:\n",
        "1. Linear regression, which will be made using the [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) library\n",
        "2. XGBoost model, based on the [XGBoost](https://xgboost.readthedocs.io/en/stable/) library. This is a type of ensemble method, similar to random forest, but includes features such as efficient regularization, parallel processing, and sparse data handling\n",
        "3. SAPS-II, which is a probabilistic model and does not require any additional training, as the disease mortality probability is included in the dataset\n",
        "\n",
        "Models 1 and 3 (Linear Regression and SAPS-II) are considered \"traditional\" approaches in the paper for predicting ICU mortality rates within timeframes. Often times, a certain threshold is applied to SAPS-II scores, but this paper does not reference one.\n",
        "The goal of the study is to show that more advanced machine learning approaches, such as XGBoost, can outperform traditional methods.\n",
        "\n",
        "\n",
        "### Model architecture\n",
        "This study focuses heavily on standard approches and evaluation vs newer, less-tested approaches (XGBoost). However, there is little mentioned about model architecture details.\n",
        "1. For the linear regression model, the formula is very basic and has little tuning involved, so no details are specified or changed.\n",
        "2. For the XGBoost model, there are quite a few architecture parameters that can be changed, such as:\n",
        "  * n_estimators - the number of trees in the model\n",
        "  * max_depth - maximum depth of trees, which helps control how specialized trees are in the ensemble\n",
        "  * learning rate - the eta parameter, this controls how much each tree contributes to the overall model\n",
        "\n",
        "  However, within the paper, there are no architecture parameters specified. Instead of trying to give an advantage to the XGBoost model via hyperparameters that were not specified in this paper, we will assume all default values for the model to compare the base-state of the XGBoost model to linear regression and SAPS-II results.\n",
        "3. For the SAPS-II model, the probability is calculated based on Simplified Acute Physiology Score II, which is a measure of patient severity of illness. This score ranged from 0 and 163, with a predicted mortality of 0 to 1, with 1 being certain death. We will be using this predicted mortality for our model.\n",
        "\n",
        "### Training objectives\n",
        "1. Linear regression has a non-customizable loss function based on the linear regression equation and we won't be customizing any loss hyperparameters\n",
        "2. For XGBoost, we will be using the default squarederror loss, as we do not want to make hyperparameter assumptions the authors didn't inform the readers of. This includes other hyperparameters including optimizers, learning steps, minimum loss, etc.\n",
        "3. SAPS-II is a probabilistic model and has no training involved\n",
        "\n",
        "\n",
        "### Computation requirements\n",
        "There are limited computational requirements for these models - the unchanged regressors have very limited number of weights, as well as hyperparameters. The number of trees, max depth, and other parameters discussed earlier, when left unchanged, have minimal requirements for the model. Any modern computer can host these models. The largest requirements come from being able to hold the data, which is done via bigquery. The dataframes are not holding much."
      ],
      "metadata": {
        "id": "ICN-hC7NGkJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "0ynkQIghddzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "\n",
        "# Set parameters for logistic regression\n",
        "params_logreg = {}\n",
        "params_logreg['penalty'] = 'l2'\n",
        "params_logreg['n_jobs'] = None\n",
        "params_logreg['random_state'] = RANDOM_STATE\n",
        "\n",
        "# Create parameters for XGBoost\n",
        "params_xgb = {}\n",
        "params_xgb['booster'] = 'gbtree'\n",
        "params_xgb['objective'] = 'binary:logistic'\n",
        "params_xgb[\"eval_metric\"] = \"auc\"\n",
        "params_xgb['eta'] = 0.1\n",
        "params_xgb['gamma'] = 0\n",
        "params_xgb['max_depth'] = 6\n",
        "params_xgb['min_child_weight']=1\n",
        "params_xgb['max_delta_step'] = 0\n",
        "params_xgb['subsample']= 1\n",
        "params_xgb['colsample_bytree']=1\n",
        "params_xgb['silent'] = 1\n",
        "params_xgb['seed'] = 0\n",
        "params_xgb['base_score'] = 0.5\n",
        "params_xgb['silent'] = 1\n",
        "params_xgb['random_state'] = RANDOM_STATE"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models\n",
        "model_logreg = LogisticRegression(**params_logreg)\n",
        "model_xgb = xgb.XGBRegressor(**params_xgb)"
      ],
      "metadata": {
        "id": "wqkaL0YZlXl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models\n",
        "model_logreg.fit(X_train_logreg, y_train_logreg)\n",
        "model_xgb.fit(X_train_xgb, y_train_xgb)"
      ],
      "metadata": {
        "id": "gtdBzrVyLt0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "For evaluation in this study, the authors used AUROC and DCA curves for comparisons. Additionally, we are going to use standard metrics, including:\n",
        "\n",
        "* accuracy\n",
        "* precision\n",
        "* recall\n",
        "* support\n",
        "* F1\n",
        "* confusion matrix\n",
        "\n",
        "The reason for these metrics is they can provide a general comparison and results analysis for our 3 models, and can help us understand the performance of each across certain metrics. Understanding accuracy, precision, sensitivity, recall, etc shows different strengths and weaknesses in each model.\n",
        "\n",
        "For our SAPS-II predictive model, a threshold must be defined for mortality classifications because this is a probabilistic model. The author's do not provide one, so we will assume that anything greater than or equal to probability 0.5 predicts \"1\", and anything less than 0.5 predicts \"0\".\n",
        "\n"
      ],
      "metadata": {
        "id": "Spx2wfhbdgq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Get probabilistic predictions from each model and use these\n",
        "predict_logreg = model_logreg.predict_proba(X_test_logreg)[:, 1]\n",
        "predict_xgb = model_xgb.predict(X_test_xgb)\n",
        "predict_sapsii = y_pred_sapsii\n",
        "\n",
        "# Get classifications for each\n",
        "predict_logreg_clf = np.where(predict_logreg >= 0.5, 1, 0)\n",
        "predict_xgb_clf = np.where(predict_xgb >= 0.5, 1, 0)\n",
        "predict_sapsii_clf = np.where(predict_sapsii >= 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "WY6THtRXxuna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get precision, recall, and macro fbeta scores\n",
        "logreg_metrics = list(metrics.precision_recall_fscore_support(y_test_logreg, predict_logreg_clf, average='macro'))[:-1]\n",
        "xgb_metrics = list(metrics.precision_recall_fscore_support(y_test_xgb, predict_xgb_clf, average='macro'))[:-1]\n",
        "sapsii_metrics = list(metrics.precision_recall_fscore_support(y_true_sapsii, predict_sapsii_clf, average='macro'))[:-1]\n",
        "\n",
        "# Get accuracy scores\n",
        "logreg_metrics.append(metrics.accuracy_score(y_test_logreg, predict_logreg_clf))\n",
        "xgb_metrics.append(metrics.accuracy_score(y_test_xgb, predict_xgb_clf))\n",
        "sapsii_metrics.append(metrics.accuracy_score(y_true_sapsii, predict_sapsii_clf))\n",
        "\n",
        "# Get confusion matrix\n",
        "cm_logreg = metrics.confusion_matrix(y_test_logreg, predict_logreg_clf)\n",
        "cm_xgb = metrics.confusion_matrix(y_test_xgb, predict_xgb_clf)\n",
        "cm_sapsii = metrics.confusion_matrix(y_true_sapsii, predict_sapsii_clf)"
      ],
      "metadata": {
        "id": "S9EeK9zssH0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get ROC curves\n",
        "fpr_logreg, tpr_logreg, _ = metrics.roc_curve(y_test_logreg, predict_logreg)\n",
        "fpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test_xgb, predict_xgb)\n",
        "fpr_sapsii, tpr_sapsii, _ = metrics.roc_curve(y_true_sapsii, y_pred_sapsii)\n",
        "\n",
        "# Get AUC scores\n",
        "logreg_metrics.append(metrics.roc_auc_score(y_test_logreg, predict_logreg))\n",
        "xgb_metrics.append(metrics.roc_auc_score(y_test_xgb, predict_xgb))\n",
        "sapsii_metrics.append(metrics.roc_auc_score(y_true_sapsii, y_pred_sapsii))"
      ],
      "metadata": {
        "id": "0iKgMMFWds4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the DCA for the models\n",
        "\n",
        "# Calculate the net benefit based on a given threshold\n",
        "def net_benefit(tp, fp, tn, fn, threshold):\n",
        "    benefit = tp - (fp * threshold / (1 - threshold))\n",
        "    total = tp + fn  # Total number of actual positives\n",
        "    return benefit / total\n",
        "\n",
        "# DCA\n",
        "def decision_curve_analysis(y_true, y_prob):\n",
        "    thresholds = np.linspace(0.01, 0.99, 100)\n",
        "    net_benefits = []\n",
        "    for threshold in thresholds:\n",
        "      y_pred = np.where(y_prob >= threshold, 1, 0)\n",
        "      tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
        "      nb = net_benefit(tp, fp, tn, fn, threshold)\n",
        "      net_benefits.append(nb)\n",
        "\n",
        "    return thresholds, net_benefits"
      ],
      "metadata": {
        "id": "xzpFkTwiwX2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at results, the XGBoost model performs slightly better than the logistic regression model. Through research, the deterministic nature of XGBoost can usually lead to repeated results, although results have varied slightly in the past.\n",
        "\n",
        "For `precision`, `recall`, `f1`, `accuracy`, and `AUC`, XGBoost performs the best, with `recall`, `f1`, and `accuracy` being a sizable difference, and `precision` and `AUC` being similar to logistic regression.\n",
        "\n",
        "For the SAPS-II model, all metrics were signficicantly outperformed by the Logistic Regression model, and the XGBoost model. The ROC curve for the SAPS-II model also indicated inferior predictive performance compared to the other models."
      ],
      "metadata": {
        "id": "eoTsx2ydhLkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results for metric scores as a dataframe\n",
        "# Create dict out of these\n",
        "data = {'metric': ['precision', 'recall', 'f1', 'accuracy', 'AUC'], 'LogReg': logreg_metrics, 'XGBoost': xgb_metrics, 'SAPS-II': sapsii_metrics}\n",
        "df_metrics = pd.DataFrame(data).round(4)\n",
        "df_metrics.set_index('metric', drop=True, inplace=True)\n",
        "\n",
        "# Display data\n",
        "display(df_metrics)"
      ],
      "metadata": {
        "id": "TsCYSp1wtR0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROC Curve\n",
        "\n",
        "Looking at the ROC curves below for each model type, we can see the difference at each threshold between models. The XGBoost model indicates better performance at lower thresholds, but is evenly matched, and sometimes even outperformed, by the Logistic Regression model, at mid-to-higher thresholds. The SAPS-II model indicated average-to-poor predictive performance, providing a minimal ROC curve more closely matching a linear increase."
      ],
      "metadata": {
        "id": "nLYs2F7ejJhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot AUROC\n",
        "plt.figure()\n",
        "plt.plot(fpr_logreg, tpr_logreg, label='LogReg')\n",
        "plt.plot(fpr_xgb, tpr_xgb, label='XGBoost')\n",
        "plt.plot(fpr_sapsii, tpr_sapsii, label='SAPS-II')\n",
        "\n",
        "# Create plot\n",
        "plt.title('AUROC')\n",
        "plt.xlabel('FP Rate')\n",
        "plt.ylabel('TP Rate')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DCA Scores\n",
        "\n",
        "DCA, or decision-curve-analysis, is used to compare clinical usefulness and net benefits between each model. The analysis helps in understanding tradeoffs between the benefit of true positive predictions, and the harm of false positive predictions within a clinical context.\n",
        "\n",
        "From the DCA curve below, we can see that the Logistic Regression model and XGBoost model perform similarly at each threshold, with some thresholds showing the XGBoost model outperforming. The SAPS-II graph has poor results here, showing low benefit at each threshold."
      ],
      "metadata": {
        "id": "28esqrDAj93I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actually calculate the DCA scores\n",
        "thresholds_logreg, net_benefits_logreg = decision_curve_analysis(y_test_logreg, predict_logreg)\n",
        "thresholds_xgb, net_benefits_xgb = decision_curve_analysis(y_test_xgb, predict_xgb)\n",
        "thresholds_sapsii, net_benefits_sapsii = decision_curve_analysis(y_true_sapsii, y_pred_sapsii)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(thresholds_logreg, net_benefits_logreg, label='LogReg')\n",
        "plt.plot(thresholds_xgb, net_benefits_xgb, label='XGBoost')\n",
        "plt.plot(thresholds_sapsii, net_benefits_sapsii, label='SAPS-II')\n",
        "\n",
        "plt.xlabel('Threshold Probability')\n",
        "plt.ylabel('Net Benefit')\n",
        "plt.title('Decision Curve Analysis')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QxUkKTNBtJ0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison\n",
        "\n",
        "Comparing the models created in my subsample study to the actual full study, results performed similarly for the Logistic Regression and XGBoost models. For the SAPS-II model, results were not as good, but this could be due to multiple factors that will be brought up in the \"Discussion\" section.\n",
        "\n",
        "The actual values for AUC, the ROC curve, and the DCA curves were not as impressive as the actual study - from the graphs below, I will compare the models I created, vs the models from the study. While trends were similar, results were simply better from the study. This could be due to multiple factors, such as the sample selection for my case study failed to capture certain features, or parameter selection differed due to unwritten differences the authors did not provide.\n",
        "\n",
        "The authors do not provide other summary metrics such as accuracy, precision, recall, or F1-scores, but we can see similar trends matching in this study vs from L. et al. [1]."
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparing ROC curves\n",
        "\n",
        "We can see similar differences in the Logistics Regression model vs the XGBoost model. The SAPS-II model provided outperforms from L. et al. [1] vs in this sub-study."
      ],
      "metadata": {
        "id": "Bxx6FuUZpzyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('From L. et al. [1]')\n",
        "show_image('graph-roc', resize=True)\n",
        "print('From this study')\n",
        "show_image('my-roc', resize=True)"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparing DCA curves"
      ],
      "metadata": {
        "id": "2xiuNrBwp2Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('From L. et al. [1]')\n",
        "show_image('graph-dca', resize=True)\n",
        "print('From this study')\n",
        "show_image('my-dca', resize=True)"
      ],
      "metadata": {
        "id": "MwnGkfU7pwhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nomogram\n",
        "\n",
        "The authors also provide a nomogram, which helps us see how important each feature is to sepsis mortality rates in this study. In the chart below, we can see how each feature is relevant for scoring in mortality predictions, and assign point values to different features. This is helpful in understanding how each feature plays a part in mortality scoring."
      ],
      "metadata": {
        "id": "awRYnvydqSNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('From L. et al. [1]')\n",
        "show_image('graph-nomogram', resize=True)"
      ],
      "metadata": {
        "id": "7-ziQYf_ql3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After reviewing the hypothesis in the paper, we can draw a few tentative conclusions:\n",
        "1. The XGBoost model has the potential to outperform standard models in predicting 30-day mortality for sepsis patients.\n",
        "2. Certain features have significant important in creating these mortality predictions, while others are less important or unnecessary altogether.\n",
        "\n",
        "\n",
        "### Reproducability\n",
        "The study by L. et al. [1] was reproducable to an extent - if a decision was made to invest more into creating an accurate data representation, I believe the study would be fully reproducable, consdering the study by L. et al. [1] uses the MIMIC-III dataset. However, due to the size of the dataset, and the hardware constraints coupled with timing constraints, a sample was chosen for this study that provided somewhat different results, although similar trends were seen.\n",
        "\n",
        "The SAPS-II results were fairly different from what was seen by L. et al. [1]. This could be due to multiple factors - a lot of assumptions had to be made regarding all models because there were no discussions on model creation, parameter selection, architectures chosen, probabilistic functions, preprocessing steps, etc. Because of this, the models seen in this notebook could be significantly different from the ones used by L. et al. [1]. That being said, with feature selection done already by the authors, we can at least get somewhat comparable results to what the authors had.\n",
        "\n",
        "### What was easy\n",
        "Model creation was actually fairly easy from this study - most of this is due to the fact that the models were not specified in any way, other than name. Because of this, minimal assumptions were made regarding model architecture, and training requirements were easy. Additionally, due to taking a sample of MIMIC-III data as opposed to the entire dataset, training times and requirements were minimal.\n",
        "\n",
        "Feature selection was straightforward as well - the authors described their methods for selecting features, and creating datasets based on this left minimal assumptions for recreating this study.\n",
        "\n",
        "### What was not easy\n",
        "Trying to understand what types of model choices the authors made was difficult - while actually creating and training the models was not difficult, trying to create an accurate replica was. Additionally, preprocessing raw MIMIC-III data was a difficult task, as features were called out by L. et al. [1], but having minimally interacted with MIMIC-III data, trying to scrape together the correct features and understand these features took most of the time involved in this study.\n",
        "\n",
        "Additionally, creating an accurate sample of the data, as well as hosting it in an accessable format, took planning and trial-and-error as well. Some databases and flavors of SQL do not support necessary date functions, or don't have portability between a simple notebook that can be ran anywhere.\n",
        "\n",
        "### Suggestions for the author\n",
        "The study was an excellent read on understanding mortality in ICU settings, traditional approaches on predicting 30-day mortality, and potential improvements to these predictions. What I would suggest is perhaps talking more on technical choices after feature selection - why use MIMIC-III, for example? Why the XGBoost model, instead of a random forest or some other ensemble method? With XGBoost, what hyperparameters were selected, such as learning rate, epochs, batch size, tree depth, child nodes, etc. XGBoost has a lot of customizability, which leads to some models training on the same dataset with drastically different results. Understanding the author's choices here would be helpful to any reader.\n",
        "\n",
        "Additionally, more baseline evaluation metrics would be helpful - we can see how each feature is relevant repeatedly, and see the ROC graphs and AUC scores, and the DCA analysis and nomogram and CDC graph. However, how do these models perform against one another within different metrics? Trying to understand F1-scores from my sub-study vs the actual study is somewhat fruitless, as there is no F1-score mentioned.\n",
        "\n",
        "Lastly, what thresholds were used for probabilistic predictions? For SAPS-II, for example, mortality probabilities are given, but the threshold decided for binary classification is entirely up to the author. One can assume that 0.5 is standard and was used in this case, but an explicit callout for this would change results drasitically.\n",
        "\n",
        "### Next phase\n",
        "For the final paper, there are a few additions I will be adding. In this study by L. et al. [1], a lot of emphasis is placed on feature selection. This is likely due to the near-endless amount of features that can be extrapolated from the MIMIC-III dataset. I will be performing a few ablation studies, to see how simple changes in feature selection either improve model results, or drastically worsen each model.\n",
        "\n",
        "Additionally, more time will be spent understanding the backwards-step-analysis done by the author, to try and understand the process left untold for these features. While the process is told, understanding the behind-the-scenes of this process can be helpful in improving model results for this sub-study."
      ],
      "metadata": {
        "id": "WwRvj0wdqzHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Hou, N., Li, M., He, L. et al. Predicting 30-days mortality for MIMIC-III patients with sepsis-3: a machine learning approach using XGboost. J Transl Med 18, 462 (2020). https://doi.org/10.1186/s12967-020-02620-5\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}